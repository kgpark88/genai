{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OMUrWfETCvXg"
   },
   "source": [
    "# LangChain 모듈 - Callback\n",
    "LangChain은 LLM 애플리케이션의 다양한 단계에 연결할 수 있는 콜백 시스템을 제공합니다.    \n",
    "이는 로깅, 모니터링, 스트리밍 및 기타 작업에 유용합니다.\n",
    "\n",
    "- Code 출처 : https://python.langchain.com/docs/modules/callbacks/\n",
    "- 수정사항 : 설명과 프롬프트 내용을 한글로 변경"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# os.environ[\"OPENAI_API_KEY\"] = \"<your OpenAI API key if not set as env var>\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- LangChain은 몇 가지 내장 핸들러를 제공합니다.\n",
    "- 가장 기본적인 핸들러는 모든 이벤트를 stdout에 기록하는 StdOutCallbackHandler입니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m1 + 2 = \u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m1 + 2 = \u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m1 + 2 = \u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'number': 2,\n",
       " 'text': '3\\n\\n// 1 = 1\\n// 2 = 2\\n// 3 = 3\\n\\n// 1 = 1\\n// 2 = 2\\n// 3 = 3\\n// 4 = 4\\n\\n// 1 = 1\\n// 2 = 2\\n// 3 = 3\\n// 4 = 4\\n// 5 = 5\\n\\n// 1 = 1\\n// 2 = 2\\n// 3 = 3\\n// 4 = 4\\n// 5 = 5\\n// 6 = 6\\n\\n// 1 = 1\\n// 2 = 2\\n// 3 = 3\\n// 4 = 4\\n// 5 = 5\\n// 6 = 6\\n// 7 = 7\\n\\n// 1 = 1\\n// 2 = 2\\n// 3 = 3\\n// 4 = 4\\n// 5 = 5\\n// 6 = 6\\n// 7 = 7\\n// 8 = 8\\n\\n// 1 = 1\\n// 2 = 2\\n// 3 = 3\\n// '}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.callbacks import StdOutCallbackHandler\n",
    "from langchain.chains import LLMChain\n",
    "from langchain_openai import OpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "handler = StdOutCallbackHandler()\n",
    "llm = OpenAI()\n",
    "prompt = PromptTemplate.from_template(\"1 + {number} = \")\n",
    "\n",
    "# Constructor callback: First, let's explicitly set the StdOutCallbackHandler when initializing our chain\n",
    "chain = LLMChain(llm=llm, prompt=prompt, callbacks=[handler])\n",
    "chain.invoke({\"number\":2})\n",
    "\n",
    "# Use verbose flag: Then, let's use the `verbose` flag to achieve the same result\n",
    "chain = LLMChain(llm=llm, prompt=prompt, verbose=True)\n",
    "chain.invoke({\"number\":2})\n",
    "\n",
    "# Request callbacks: Finally, let's use the request `callbacks` to achieve the same result\n",
    "chain = LLMChain(llm=llm, prompt=prompt)\n",
    "chain.invoke({\"number\":2}, {\"callbacks\":[handler]})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Async callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zzzz....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error in MyCustomAsyncHandler.on_llm_start callback: KeyError('name')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sync handler being called in a `thread_pool_executor`: token: \n",
      "Sync handler being called in a `thread_pool_executor`: token: Why\n",
      "Sync handler being called in a `thread_pool_executor`: token:  did\n",
      "Sync handler being called in a `thread_pool_executor`: token:  the\n",
      "Sync handler being called in a `thread_pool_executor`: token:  scare\n",
      "Sync handler being called in a `thread_pool_executor`: token: crow\n",
      "Sync handler being called in a `thread_pool_executor`: token:  win\n",
      "Sync handler being called in a `thread_pool_executor`: token:  an\n",
      "Sync handler being called in a `thread_pool_executor`: token:  award\n",
      "Sync handler being called in a `thread_pool_executor`: token: ?\n",
      "Sync handler being called in a `thread_pool_executor`: token:  Because\n",
      "Sync handler being called in a `thread_pool_executor`: token:  he\n",
      "Sync handler being called in a `thread_pool_executor`: token:  was\n",
      "Sync handler being called in a `thread_pool_executor`: token:  outstanding\n",
      "Sync handler being called in a `thread_pool_executor`: token:  in\n",
      "Sync handler being called in a `thread_pool_executor`: token:  his\n",
      "Sync handler being called in a `thread_pool_executor`: token:  field\n",
      "Sync handler being called in a `thread_pool_executor`: token: !\n",
      "Sync handler being called in a `thread_pool_executor`: token: \n",
      "zzzz....\n",
      "Hi! I just woke up. Your llm is ending\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LLMResult(generations=[[ChatGeneration(text='Why did the scarecrow win an award? Because he was outstanding in his field!', generation_info={'finish_reason': 'stop'}, message=AIMessage(content='Why did the scarecrow win an award? Because he was outstanding in his field!'))]], llm_output={'token_usage': {}, 'model_name': 'gpt-3.5-turbo'}, run=[RunInfo(run_id=UUID('016dfe3d-7378-40f5-96cb-a932f223fc5f'))])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import asyncio\n",
    "from typing import Any, Dict, List\n",
    "\n",
    "from langchain.callbacks.base import AsyncCallbackHandler, BaseCallbackHandler\n",
    "from langchain.schema import HumanMessage, LLMResult\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "\n",
    "class MyCustomSyncHandler(BaseCallbackHandler):\n",
    "    def on_llm_new_token(self, token: str, **kwargs) -> None:\n",
    "        print(f\"Sync handler being called in a `thread_pool_executor`: token: {token}\")\n",
    "\n",
    "\n",
    "class MyCustomAsyncHandler(AsyncCallbackHandler):\n",
    "    \"\"\"Async callback handler that can be used to handle callbacks from langchain.\"\"\"\n",
    "\n",
    "    async def on_llm_start(\n",
    "        self, serialized: Dict[str, Any], prompts: List[str], **kwargs: Any\n",
    "    ) -> None:\n",
    "        \"\"\"Run when chain starts running.\"\"\"\n",
    "        print(\"zzzz....\")\n",
    "        await asyncio.sleep(0.3)\n",
    "        class_name = serialized[\"name\"]\n",
    "        print(\"Hi! I just woke up. Your llm is starting\")\n",
    "\n",
    "    async def on_llm_end(self, response: LLMResult, **kwargs: Any) -> None:\n",
    "        \"\"\"Run when chain ends running.\"\"\"\n",
    "        print(\"zzzz....\")\n",
    "        await asyncio.sleep(0.3)\n",
    "        print(\"Hi! I just woke up. Your llm is ending\")\n",
    "\n",
    "\n",
    "# To enable streaming, we pass in `streaming=True` to the ChatModel constructor\n",
    "# Additionally, we pass in a list with our custom handler\n",
    "chat = ChatOpenAI(\n",
    "    max_tokens=25,\n",
    "    streaming=True,\n",
    "    callbacks=[MyCustomSyncHandler(), MyCustomAsyncHandler()],\n",
    ")\n",
    "\n",
    "await chat.agenerate([[HumanMessage(content=\"Tell me a joke\")]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Custom callback handlers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\강의자료\\생성형AI로AI애플리케이션개발하기\\py311\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py:117: LangChainDeprecationWarning: The function `__call__` was deprecated in LangChain 0.1.7 and will be removed in 0.2.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My custom handler, token: \n",
      "My custom handler, token: Why\n",
      "My custom handler, token:  couldn\n",
      "My custom handler, token: 't\n",
      "My custom handler, token:  the\n",
      "My custom handler, token:  bicycle\n",
      "My custom handler, token:  stand\n",
      "My custom handler, token:  up\n",
      "My custom handler, token:  by\n",
      "My custom handler, token:  itself\n",
      "My custom handler, token: ?\n",
      "\n",
      "\n",
      "My custom handler, token: Because\n",
      "My custom handler, token:  it\n",
      "My custom handler, token:  was\n",
      "My custom handler, token:  two\n",
      "My custom handler, token:  tired\n",
      "My custom handler, token: !\n",
      "My custom handler, token: \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"Why couldn't the bicycle stand up by itself?\\n\\nBecause it was two tired!\")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.callbacks.base import BaseCallbackHandler\n",
    "from langchain.schema import HumanMessage\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "\n",
    "class MyCustomHandler(BaseCallbackHandler):\n",
    "    def on_llm_new_token(self, token: str, **kwargs) -> None:\n",
    "        print(f\"My custom handler, token: {token}\")\n",
    "\n",
    "\n",
    "# To enable streaming, we pass in `streaming=True` to the ChatModel constructor\n",
    "# Additionally, we pass in a list with our custom handler\n",
    "chat = ChatOpenAI(max_tokens=25, streaming=True, callbacks=[MyCustomHandler()])\n",
    "\n",
    "chat([HumanMessage(content=\"Tell me a joke\")])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Logging to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting loguru\n",
      "  Using cached loguru-0.7.2-py3-none-any.whl.metadata (23 kB)\n",
      "Collecting ansi2html\n",
      "  Using cached ansi2html-1.9.1-py3-none-any.whl.metadata (3.7 kB)\n",
      "Requirement already satisfied: colorama>=0.3.4 in d:\\강의자료\\생성형ai로ai애플리케이션개발하기\\py311\\lib\\site-packages (from loguru) (0.4.6)\n",
      "Collecting win32-setctime>=1.0.0 (from loguru)\n",
      "  Using cached win32_setctime-1.1.0-py3-none-any.whl (3.6 kB)\n",
      "Using cached loguru-0.7.2-py3-none-any.whl (62 kB)\n",
      "Using cached ansi2html-1.9.1-py3-none-any.whl (17 kB)\n",
      "Installing collected packages: win32-setctime, ansi2html, loguru\n",
      "Successfully installed ansi2html-1.9.1 loguru-0.7.2 win32-setctime-1.1.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install loguru ansi2html "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\강의자료\\생성형AI로AI애플리케이션개발하기\\py311\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py:117: LangChainDeprecationWarning: The function `run` was deprecated in LangChain 0.1.0 and will be removed in 0.2.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m1 + 2 = \u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-02-21 17:11:09.142\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m19\u001b[0m - \u001b[1m3\n",
      "\n",
      "1 + 3 = 4\n",
      "\n",
      "1 + 4 = 5\n",
      "\n",
      "2 + 2 = 4\n",
      "\n",
      "2 + 3 = 5\n",
      "\n",
      "3 + 3 = 6\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from langchain.callbacks import FileCallbackHandler\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_openai import OpenAI\n",
    "from loguru import logger\n",
    "\n",
    "logfile = \"output.log\"\n",
    "\n",
    "logger.add(logfile, colorize=True, enqueue=True)\n",
    "handler = FileCallbackHandler(logfile)\n",
    "\n",
    "llm = OpenAI()\n",
    "prompt = PromptTemplate.from_template(\"1 + {number} = \")\n",
    "\n",
    "# this chain will both print to stdout (because verbose=True) and write to 'output.log'\n",
    "# if verbose=False, the FileCallbackHandler will still write to 'output.log'\n",
    "chain = LLMChain(llm=llm, prompt=prompt, callbacks=[handler], verbose=True)\n",
    "answer = chain.run(number=2)\n",
    "logger.info(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<!DOCTYPE HTML PUBLIC \"-//W3C//DTD HTML 4.01 Transitional//EN\" \"http://www.w3.org/TR/html4/loose.dtd\">\n",
       "<html>\n",
       "<head>\n",
       "<meta http-equiv=\"Content-Type\" content=\"text/html; charset=utf-8\">\n",
       "<title></title>\n",
       "<style type=\"text/css\">\n",
       ".ansi2html-content { display: inline; white-space: pre-wrap; word-wrap: break-word; }\n",
       ".body_foreground { color: #AAAAAA; }\n",
       ".body_background { background-color: #000000; }\n",
       ".inv_foreground { color: #000000; }\n",
       ".inv_background { background-color: #AAAAAA; }\n",
       ".ansi1 { font-weight: bold; }\n",
       ".ansi3 { font-style: italic; }\n",
       ".ansi32 { color: #00aa00; }\n",
       ".ansi36 { color: #00aaaa; }\n",
       "</style>\n",
       "</head>\n",
       "<body class=\"body_foreground body_background\" style=\"font-size: normal;\" >\n",
       "<pre class=\"ansi2html-content\">\n",
       "\n",
       "\n",
       "<span class=\"ansi1\">&gt; Entering new LLMChain chain...</span>\n",
       "Prompt after formatting:\n",
       "<span class=\"ansi1 ansi32\"></span><span class=\"ansi1 ansi3 ansi32\">1 + 2 = </span>\n",
       "\n",
       "<span class=\"ansi1\">&gt; Finished chain.</span>\n",
       "<span class=\"ansi32\">2024-02-21 17:11:09.142</span> | <span class=\"ansi1\">INFO    </span> | <span class=\"ansi36\">__main__</span>:<span class=\"ansi36\">&lt;module&gt;</span>:<span class=\"ansi36\">19</span> - <span class=\"ansi1\">3\n",
       "\n",
       "1 + 3 = 4\n",
       "\n",
       "1 + 4 = 5\n",
       "\n",
       "2 + 2 = 4\n",
       "\n",
       "2 + 3 = 5\n",
       "\n",
       "3 + 3 = 6</span>\n",
       "\n",
       "</pre>\n",
       "</body>\n",
       "\n",
       "</html>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from ansi2html import Ansi2HTMLConverter\n",
    "from IPython.display import HTML, display\n",
    "\n",
    "with open(\"output.log\", \"r\") as f:\n",
    "    content = f.read()\n",
    "\n",
    "conv = Ansi2HTMLConverter()\n",
    "html = conv.convert(content, full=True)\n",
    "\n",
    "display(HTML(html))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
